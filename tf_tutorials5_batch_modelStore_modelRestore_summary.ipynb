{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------a: Tensor(\"ParseSingleExample_1/Squeeze_a:0\", shape=(), dtype=float32)\n",
      "1.618\n",
      "[2016 2017]\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "-----batch1---------\n",
      "[ 0.61799997  0.61799997  0.61799997] [[2016 2017]\n",
      " [2016 2017]\n",
      " [2016 2017]] [[[0 1 2]\n",
      "  [3 4 5]]\n",
      "\n",
      " [[0 1 2]\n",
      "  [3 4 5]]\n",
      "\n",
      " [[0 1 2]\n",
      "  [3 4 5]]]\n",
      "-----batch2---------\n",
      "[ 0.61799997  1.61800003  0.61799997] [[2016 2017]\n",
      " [2017 2018]\n",
      " [2016 2017]] [[[0 1 2]\n",
      "  [3 4 5]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[0 1 2]\n",
      "  [3 4 5]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "def write_binary():\n",
    "    writer = tf.python_io.TFRecordWriter('D://tmp//data.tfrecord')\n",
    "\n",
    "    for i in range(0, 2):\n",
    "        a = 0.618 + i\n",
    "        b = [2016 + i, 2017+i]\n",
    "        c = numpy.array([[0, 1, 2],[3, 4, 5]]) + i\n",
    "        c = c.astype(numpy.uint8)\n",
    "        c_raw = c.tostring()\n",
    "\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    'a': tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(value=[a])\n",
    "                    ),\n",
    "\n",
    "                    'b': tf.train.Feature(\n",
    "                        int64_list=tf.train.Int64List(value=b)\n",
    "                    ),\n",
    "                    'c': tf.train.Feature(\n",
    "                        bytes_list=tf.train.BytesList(value=[c_raw])\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "def read_single_sample(filename):\n",
    "    # output file name string to a queue\n",
    "    filename_queue = tf.train.string_input_producer([filename], num_epochs=None)\n",
    "\n",
    "    # create a reader from file queue\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    # get feature from serialized example\n",
    "\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'a': tf.FixedLenFeature([], tf.float32),\n",
    "            'b': tf.FixedLenFeature([2], tf.int64),\n",
    "            'c': tf.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    a = features['a']\n",
    "\n",
    "    b = features['b']\n",
    "\n",
    "    c_raw = features['c']\n",
    "    c = tf.decode_raw(c_raw, tf.uint8)\n",
    "    c = tf.reshape(c, [2, 3])\n",
    "\n",
    "    return a, b, c\n",
    "\n",
    "#-----main function-----\n",
    "sess = tf.InteractiveSession()\n",
    "write_binary()\n",
    "\n",
    "# create tensor\n",
    "a, b, c = read_single_sample('d://tmp//data.tfrecord')\n",
    "print('---------------a:',a)\n",
    "a_batch, b_batch, c_batch = tf.train.shuffle_batch([a, b, c], batch_size=3, capacity=200, min_after_dequeue=100, num_threads=2)\n",
    "\n",
    "queues = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)\n",
    "\n",
    "# sess\n",
    "#sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "print(a.eval())\n",
    "print(b.eval())\n",
    "print(c.eval())\n",
    "a_val, b_val, c_val = sess.run([a_batch, b_batch, c_batch])\n",
    "print('-----batch1---------')\n",
    "print(a_val, b_val, c_val)\n",
    "a_val, b_val, c_val = sess.run([a_batch, b_batch, c_batch])\n",
    "print('-----batch2---------')\n",
    "print(a_val, b_val, c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.name_scope('scope_a'):\n",
    "    a = tf.add(1,2,name = 'a_add')\n",
    "    b = tf.multiply(a,3,name='a_mul')\n",
    "with tf.name_scope('scope_c'):\n",
    "    c = tf.add(4,5,name='c_add')\n",
    "    d = tf.multiply(c,6,name='c_mul')\n",
    "e = tf.add(b,d,name='output')\n",
    "\n",
    "writer = tf.summary.FileWriter('d://tmp//name_scope_1',graph = tf.get_default_graph())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.08\n",
      "step 10, training accuracy 0.26\n",
      "step 20, training accuracy 0.54\n",
      "step 30, training accuracy 0.42\n",
      "step 40, training accuracy 0.58\n",
      "step 50, training accuracy 0.58\n",
      "step 60, training accuracy 0.84\n",
      "step 70, training accuracy 0.82\n",
      "step 80, training accuracy 0.76\n",
      "step 90, training accuracy 0.72\n",
      "step 100, training accuracy 0.84\n",
      "step 110, training accuracy 0.84\n",
      "step 120, training accuracy 0.78\n",
      "step 130, training accuracy 0.94\n",
      "step 140, training accuracy 0.76\n",
      "test accuracy 0.884\n"
     ]
    }
   ],
   "source": [
    "#----------------------simple cnn\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot = True)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#########定义初始化函数###############\n",
    "def weight_variable(shape): #先定义好初始化函数以便重复使用。我们需要给权重制造一些随机噪声来打破完全对称\n",
    "    initial = tf.truncated_normal(shape,stddev = 0.1) #返回A tensor of the specified shape filled with random truncated normal values.\n",
    "    return tf.Variable(initial)  #返回一个Tensor.Returns the Tensor used as the initial value for the variable.\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape = shape) #返回A Constant Tensor.\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "#########定义神经网络中共的核心函数###############\n",
    "def conv2d(x,W): #W卷积参数[5,5,1,32] 5*5代表卷积核尺寸；1：代表通道数,灰度图像为1，RGB图像为3；32卷积核数目\n",
    "    return tf.nn.conv2d(x,W,strides = [1,1,1,1],padding = \"SAME\") #返回A Tensor. Has the same type as input\n",
    "def max_pool_2_2(x): #这里使用2*2的最大池化，即将一个2*2变成1*1.因为希望整体缩小图片尺寸，所以strides设置为横竖两个方向以2为步长。\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides = [1,2,2,1],padding = \"SAME\") #返回A Tensor with type tf.float32. The max pooled output tensor.\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "#########在正式设计神经网络之前先定义输入的placeholder函数###########\n",
    "x = tf.placeholder(tf.float32,[None,784])  #输入样本参量\n",
    "y_ = tf.placeholder(tf.float32,[None,10])  #输入样本标签,输出A Tensor that may be used as a handle for feeding a value, but not evaluated directly.\n",
    "x_image = tf.reshape(x,[-1,28,28,1])#-1代表样本数量不确定，每个样本28*28，1代表颜色通道数\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "###############定义第一个卷积层#####################################\n",
    "W_conv1 = weight_variable([5,5,1,32]) #卷积核尺寸5*5,1个颜色通道，32个不同卷积核,感觉像高维向量\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2_2(h_conv1)\n",
    "###############定义第二个卷积层#####################################\n",
    "W_conv2 = weight_variable([5,5,32,64]) #卷积核尺寸5*5,注意这里变为32个通道，因为第一层输出32个卷积核结果，64个不同卷积核,感觉像高维向量\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2_2(h_conv2)\n",
    "###############最后跟一个全连接层####################\n",
    "#前面经过两次池化，所以边长只有原来1/4了，图片尺寸为7*7，第二层卷积核数量为64，其输出tensor为7*7*64,隐含节点数为1024。\n",
    "W_fc1 = weight_variable([7*7*64,1024])  #不知道这里7*7*64怎么来的\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)\n",
    "##################加一个Dropout层####################\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "##################最后将Dropout层连接一个Softmax层########\n",
    "W_fc2 = weight_variable([1024,10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "###########定义损失函数,########################\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv),reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "##########评测准确率################################\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#########################训练################\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(150):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%10 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict = {x:batch[0],y_:batch[1],keep_prob:1.0})\n",
    "        print(\"step %d, training accuracy %g\" %(i,train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})\n",
    "    \n",
    "print(\"test accuracy %g\" %accuracy.eval(feed_dict = {x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0}))\n",
    "#eval参见 tf.Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------生成logs文件夹（tensorboard）\n",
    "# coding=utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "首先载入Tensorflow，并设置训练的最大步数为1000,学习率为0.001,dropout的保留比率为0.9。\n",
    "同时，设置MNIST数据下载地址data_dir和汇总数据的日志存放路径log_dir。\n",
    "这里的日志路径log_dir非常重要，会存放所有汇总数据供Tensorflow展示。\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "max_step = 1000\n",
    "learning_rate = 0.001\n",
    "dropout = 0.9\n",
    "data_dir = 'MNIST_data/'\n",
    "log_dir = 'logs/'\n",
    "\n",
    "# 使用input_data.read_data_sets下载MNIST数据，并创建Tensorflow的默认Session\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "\"\"\"\n",
    "为了在TensorBoard中展示节点名称，设计网络时会常使用tf.name_scope限制命名空间，\n",
    "在这个with下所有的节点都会自动命名为input/xxx这样的格式。\n",
    "定义输入x和y的placeholder，并将输入的一维数据变形为28×28的图片存储到另一个tensor，\n",
    "这样就可以使用tf.summary.image将图片数据汇总给TensorBoard展示了。\n",
    "\"\"\"\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x_input')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y_input')\n",
    "\n",
    "with tf.name_scope('input_reshape'):\n",
    "    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "# 定义神经网络模型参数的初始化方法，\n",
    "# 权重依然使用常用的truncated_normal进行初始化，偏置则赋值为0.1\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 定义对Variable变量的数据汇总函数\n",
    "\"\"\"\n",
    "计算出Variable的mean,stddev,max和min，\n",
    "对这些标量数据使用tf.summary.scalar进行记录和汇总。\n",
    "同时，使用tf.summary.histogram直接记录变量var的直方图。\n",
    "\"\"\"\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "# 设计一个MLP多层神经网络来训练数据，在每一层中都会对模型参数进行数据汇总。\n",
    "\"\"\"\n",
    "定一个创建一层神经网络并进行数据汇总的函数nn_layer。\n",
    "这个函数的输入参数有输入数据input_tensor,输入的维度input_dim,输出的维度output_dim和层名称layer_name，激活函数act则默认使用Relu。\n",
    "在函数内，显示初始化这层神经网络的权重和偏置，并使用前面定义的variable_summaries对variable进行数据汇总。\n",
    "然后对输入做矩阵乘法并加上偏置，再将未进行激活的结果使用tf.summary.histogram统计直方图。\n",
    "同时，在使用激活函数后，再使用tf.summary.histogram统计一次。\n",
    "\"\"\"\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name,act=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weight'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='actvations')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return activations\n",
    "\n",
    "\"\"\"\n",
    "使用刚定义好的nn_layer创建一层神经网络，输入维度是图片的尺寸（784=24×24），输出的维度是隐藏节点数500.\n",
    "再创建一个Droput层，并使用tf.summary.scalar记录keep_prob。然后再使用nn_layer定义神经网络的输出层，激活函数为全等映射，此层暂时不使用softmax,在后面会处理。\n",
    "\"\"\"\n",
    "hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "    dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "y1 = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "\n",
    "\"\"\"\n",
    "这里使用tf.nn.softmax_cross_entropy_with_logits()对前面输出层的结果进行softmax处理并计算交叉熵损失cross_entropy。\n",
    "计算平均损失，并使用tf.summary.saclar进行统计汇总。\n",
    "\"\"\"\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits(logits=y1, labels=y)\n",
    "    with tf.name_scope('total'):\n",
    "        cross_entropy = tf.reduce_mean(diff)\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "\"\"\"\n",
    "使用Adma优化器对损失进行优化，同时统计预测正确的样本数并计算正确率accuray，\n",
    "再使用tf.summary.scalar对accuracy进行统计汇总。\n",
    "\"\"\"\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y1, 1), tf.arg_max(y, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "\"\"\"\n",
    "由于之前定义了非常多的tf.summary的汇总操作，一一执行这些操作态麻烦，\n",
    "所以这里使用tf.summary.merger_all()直接获取所有汇总操作，以便后面执行。\n",
    "然后，定义两个tf.summary.FileWrite(文件记录器)在不同的子目录，分别用来存放训练和测试的日志数据。\n",
    "同时，将Session的计算图sess.graph加入训练过程的记录器，这样在TensorBoard的GRAPHS窗口中就能展示整个计算图的可视化效果。\n",
    "最后使用tf.global_variables_initializer().run()初始化全部变量。\n",
    "\"\"\"\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(log_dir + '/test')\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\"\"\"\n",
    "定义feed_dict的损失函数。\n",
    "该函数先判断训练标记，如果训练标记为true,则从mnist.train中获取一个batch的样本，并设置dropout值;\n",
    "如果训练标记为False，则获取测试数据，并设置keep_prob为1,即等于没有dropout效果。\n",
    "\"\"\"\n",
    "def feed_dict(train):\n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(100)\n",
    "        k = dropout\n",
    "    else:\n",
    "        xs, ys = mnist.test.images, mnist.test.labels\n",
    "        k = 1.0\n",
    "    return {x: xs, y: ys, keep_prob: k}\n",
    "\n",
    "# 实际执行具体的训练，测试及日志记录的操作\n",
    "\"\"\"\n",
    "首先，使用tf.train.Saver()创建模型的保存器。\n",
    "然后，进入训练的循环中，每隔10步执行一次merged（数据汇总），accuracy（求测试集上的预测准确率）操作，\n",
    "并使应test_write.add_summary将汇总结果summary和循环步数i写入日志文件;\n",
    "同时每隔100步，使用tf.RunOption定义Tensorflow运行选项，其中设置trace_level为FULL——TRACE,\n",
    "并使用tf.RunMetadata()定义Tensorflow运行的元信息，\n",
    "这样可以记录训练是运算时间和内存占用等方面的信息.\n",
    "再执行merged数据汇总操作和train_step训练操作，将汇总summary和训练元信息run_metadata添加到train_writer.\n",
    "平时，则执行merged操作和train_step操作，并添加summary到trian_writer。\n",
    "所有训练全部结束后，关闭train_writer和test_writer。\n",
    "\"\"\"\n",
    "saver = tf.train.Saver()\n",
    "for i in range(max_step):\n",
    "    if i % 10 == 0:\n",
    "        summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "        test_writer.add_summary(summary, i)\n",
    "        print('Accuracy at step %s: %s' % (i, acc))\n",
    "    else:\n",
    "        if i % 100 == 99:\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True),\n",
    "                                  options=run_options, run_metadata=run_metadata)\n",
    "            train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "            train_writer.add_summary(summary, i)\n",
    "            saver.save(sess, log_dir+\"/model.ckpt\", i)\n",
    "            print('Adding run metadata for', i)\n",
    "        else:\n",
    "            summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "            train_writer.add_summary(summary, i)\n",
    "train_writer.close()\n",
    "test_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./model/model_name-1000'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------载入模型例1\n",
    "import tensorflow as tf\n",
    "#Prepare to feed input, i.e. feed_dict and placeholders\n",
    "w1 = tf.placeholder(\"float\", name=\"w1\")\n",
    "w2 = tf.placeholder(\"float\", name=\"w2\")\n",
    "b1= tf.Variable(2.0,name=\"bias\")\n",
    "feed_dict ={w1:4,w2:8}\n",
    "\n",
    "#Define a test operation that we will restore\n",
    "w3 = tf.add(w1,w2)\n",
    "w4 = tf.multiply(w3,b1,name=\"op_to_restore\")\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Create a saver object which will save all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#Run the operation by feeding input\n",
    "print(sess.run(w4,feed_dict))\n",
    "\n",
    "#Prints 24 which is sum of (w1+w2)*b1 \n",
    "#Now, save the graph\n",
    "saver.save(sess, './model/model_name',global_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_name-1000\n",
      "载入模型后   v:\n",
      "[<tf.Variable 'bias:0' shape=() dtype=float32_ref>]\n",
      "2.0\n",
      "定义w1 w2   v:\n",
      "[<tf.Variable 'bias:0' shape=() dtype=float32_ref>]\n",
      "60.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess=tf.Session()    \n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('./model/model_name-1000.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./model/'))\n",
    "\n",
    "#---------------\n",
    "v = tf.global_variables()\n",
    "print('载入模型后   v:')\n",
    "print(v)\n",
    "\n",
    "# Access saved Variables directly\n",
    "print(sess.run('bias:0'))\n",
    "# This will print 2, which is the value of bias that we saved\n",
    "# Now, let's access and create placeholders variables and\n",
    "# create feed-dict to feed new data\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "w2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "feed_dict ={w1:13.0,w2:17.0}\n",
    "\n",
    "#---------------\n",
    "v = tf.global_variables()\n",
    "print('定义w1 w2   v:')\n",
    "print(v)\n",
    "\n",
    "#Now, access the op that you want to run. \n",
    "op_to_restore = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    "print(sess.run(op_to_restore,feed_dict))\n",
    "#This will print 60 which is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[<tf.Variable 'Variable:0' shape=() dtype=int32_ref>, <tf.Variable 'Variable_1:0' shape=() dtype=int32_ref>, <tf.Variable 'a:0' shape=() dtype=int32_ref>, <tf.Variable 'b:0' shape=() dtype=int32_ref>, <tf.Variable 'a_1:0' shape=() dtype=int32_ref>, <tf.Variable 'b_1:0' shape=() dtype=int32_ref>]\n"
     ]
    }
   ],
   "source": [
    "#-----------------tf.global_variables\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "a = tf.Variable(4,tf.float32,name='a')\n",
    "b = tf.Variable(3,name='b')\n",
    "tf.global_variables_initializer().run()\n",
    "print(a.eval())\n",
    "\n",
    "v = tf.global_variables()\n",
    "print(v)\n",
    "#[<tf.Variable 'Variable:0' shape=() dtype=int32_ref>, <tf.Variable 'Variable_1:0' shape=() dtype=int32_ref>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n",
      "Model stored....\n"
     ]
    }
   ],
   "source": [
    "#---------------载入模型例2\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "v1 = tf.Variable(1.1, name=\"v1\")\n",
    "v2 = tf.Variable(1.2, name=\"v2\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print (v2.eval(sess))\n",
    "    save_path=\"./model2/model_name\"\n",
    "    saver.save(sess,save_path,global_step=1000)\n",
    "    print (\"Model stored....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model2/\n",
      "[<tf.Variable 'v1:0' shape=() dtype=float32_ref>, <tf.Variable 'v2:0' shape=() dtype=float32_ref>]\n",
      "Model restored.\n",
      "1.1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "v3 = tf.Variable(3.0, name=\"v1\")\n",
    "v4 = tf.Variable(8.0, name=\"v2\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    save_path=\"./model2/\"\n",
    "\n",
    "    saver.restore(sess,save_path)#tf.train.latest_checkpoint('./model2/')\n",
    "    v = tf.global_variables()\n",
    "    print(v)\n",
    "    print(\"Model restored.\")\n",
    "    print (sess.run(v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot = True)    #导入数据\n",
    "sess = tf.InteractiveSession()    #开启一个Session\n",
    "\n",
    "def weight_variable(shape):    #定义一个权重变量\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)    #初始化一个正态分布噪声，标准差为0.1\n",
    "    return tf.Variable(initial)    \n",
    "\n",
    "def bias_variable(shape):    #定义一个偏置变量\n",
    "    initial = tf.constant(0.1,shape=shape)    #初始化一个常量，取值为0.1，用来避免死亡节点\n",
    "    return tf.Variable(initial)    \n",
    "\n",
    "def conv2d(x,W):    #定义一个卷积层，输入参数为（输入向量X，权重向量W）\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')    #定义一个二维卷积层，\n",
    "\n",
    "def max_pool_2x2(x):    #定义一个池化层\n",
    "        return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')    #调用TF的池化层函数\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,784])    #定义输入，条目不限，维度为784\n",
    "y_ = tf.placeholder(tf.float32,[None,10])    #定义输入，条目不限，类别为10\n",
    "x_image = tf.reshape(x,[-1,28,28,1])     #对1*785的数据进行reshape，转化为二维数据，-1代表样本不固定，尺寸转化为28*28，通道为1\n",
    "\n",
    "W_conv1 = weight_variable([5,5,1,32])    #初始化第一个卷积层的权重变量，5*5的核，1个通道，32个不同的卷积核\n",
    "b_conv1 = bias_variable([32])    #初始化对应于卷积核的偏置\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)    #调用卷积层定义函数，初始化第一个卷积层的激活函数，使用relu激活函数\n",
    "h_pool1 = max_pool_2x2(h_conv1)    #调用池化层定义函数，初始化第一个卷积层之后的池化层\n",
    "\n",
    "W_conv2 = weight_variable([5,5,32,64])    #初始化第二个卷积层的权重变量，5*5的核，来自上一层的32个通道，64个不同的卷积核\n",
    "b_conv2 = bias_variable([64])    #对应于64个核的偏置\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)   \n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7*7*64,1024])    #初始化第一个全连接层，大小变为7*7*64，共有1024个隐含节点\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)    #定义dropout的placeholder\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)    #定义dropout层，输入是fc1层输出结果以及keep_prob\n",
    "\n",
    "W_fc2 = weight_variable([1024,10])    \n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)    #上一层dropout的输出连接一个softmax层\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y_conv),reduction_indices=[1]))    #求交叉熵\n",
    "tf.summary.scalar('cross_entropy_name',cross_entropy)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)    #使用Adm优化器求得最小的交叉熵\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))    #比较预测结果与真实标签准确率\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))    #求准确率\n",
    "\n",
    "#-----------summary.scalar函数生成summary protobuf\n",
    "tf.summary.scalar('accuracy_name',accuracy)\n",
    "#tf.summary.scalar('cross_entropy',cross_entropy)\n",
    "\n",
    "\n",
    "#-----------生成saver\n",
    "saver = tf.train.Saver()\n",
    "#--------------- 3.定义一个summury op, 用来汇总由scalar_summary记录的所有变量\n",
    "#merged_summary_op = tf.merge_all_summaries()\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#-------------- 4.生成一个summary writer对象，需要指定写入路径,例如我这边就是/tmp/logdir\n",
    "summary_writer = tf.summary.FileWriter('./logdir', sess.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "for i in range(1000):    #进行20000次训练迭代\n",
    "    batch = mnist.train.next_batch(50)    #使用大小为50的mini-batch\n",
    "    if i%50==0:    #每迭代100次打印一次\n",
    "        #train_accuracy = accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.0})     #评测时，dropout比例为1.0\n",
    "        train_accuracy,summary_str = sess.run([accuracy, merged_summary_op],feed_dict={x:batch[0],y_:batch[1],keep_prob:1.0})\n",
    "        \n",
    "        summary_writer.add_summary(summary_str,i)\n",
    "        print(\"step %d ,training accuracy %g\" %(i,train_accuracy))\n",
    "        \n",
    "    train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})    #训练时，dropout比例为0.5\n",
    "\n",
    "\n",
    "\n",
    "#------------训练完成后，保存模型\n",
    "saver.save(sess,'./model_simple_cnn/model_name')\n",
    "\n",
    "print(\"test accuracy %g\" %accuracy.eval(feed_dict={x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0}))    #评测时，dropout比例为1.0\n",
    "\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
